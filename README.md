# курс [OTUS Deep Learning](https://otus.ru/lessons/dl-basic/)

## Задание по теме №4 - Numpy

### Цель:
Реализовать метод стохастического градиентного спуска на примере задачи логистической регрессии.

### Задания:
0. Используя np.random научиться сэмплировать случайные точки из двумерного нормального распределения. В качестве параметров двух разных нормальных распределений взять mu_0=(-1, -1) и mu_1 = (1, 1), с единичными дисперсиями.
1. Заменить функцию ошибки MSE на LogLoss.
2. Обучить модель на примерах из этих распределений "предсказывать" 0 и 1 соответственно для первого и второго распределений.
3. Используя библиотеку matplotlib нарисовать разделяющую поверхность.
4. Заменить однослойную нейронную сеть двухслойной.

### Дополнительные материалы:
[Ничего, кроме NumPy: понимание и создание нейронных сетей двоичной классификации с помощью вычислительных графов с нуля](https://disk.yandex.ru/i/8cx5ZwqbZ8kplw)

[Cross Entropy Loss Error Function - ML for beginners!](https://www.youtube.com/watch?v=EJRFP3WmS6Q)

[Рисуем разделительную линию](http://recog.ru/%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D1%8B-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B-%D1%81-tensorflow-%D0%B8-keras-%D0%BD%D0%B0-python-%D1%87%D0%B0%D1%81%D1%82%D1%8C-1/)

